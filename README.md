# BitGen: Advanced Tiny Language Model for Embedded Systems

An advanced tiny language model that integrates **Larimar Episodic Memory**, **BitNet 1.58-bit Quantization**, **FIBER Cross-Modal Fusion**, **Attention Sinks**, **Tiny-R1 Reasoning**, and **Robot Selection** capabilities.

## ğŸš€ Quick Start: Training BitGen

### Prerequisites
```bash
# Install dependencies
pip install -r requirements.txt

# Download COCO dataset
python download_coco_dataset.py
```

### Stage 1: Vision-Language Pre-training
Train stable vision-language representations using COCO dataset (NO reasoning yet):

```bash
cd src
python train_stage1_vision_language.py
```

**What Stage 1 does:**
- FIBER-style cross-modal fusion with queue-based contrastive learning (4096 queues)
- Larima GPM episodic memory training (Bayesian inference)
- DINOv2 vision encoder (trainable, end-to-end)
- Contrastive accuracy target: >40% (imageâ†”text matching)
- Output: `checkpoints/stage1/stage1_best.pt`

**Training time:** ~5-10 hours on A40 GPU (10 epochs, COCO 307K samples)

### Stage 2: Reasoning Module Training
Load Stage 1 checkpoint and add reasoning on top:

```bash
cd src
python train_stage2_reasoning.py
```

**What Stage 2 does:**
- Loads frozen Stage 1 vision-language base
- Adds tiny-r1 style reasoning module (chain-of-thought)
- GRPO training with reward functions (correctness, trace quality)
- Robot selection dataset (multi-label classification)
- Output: `checkpoints/stage2/stage2_best.pt`

**Training time:** ~2-4 hours on A40 GPU (20 epochs, Robot dataset ~1K samples)

### Dataset Structure
Your directory should look like:
```
BitGen/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ coco/
â”‚       â”œâ”€â”€ validated_coco.json  # Validated COCO dataset (use this!)
â”‚       â”œâ”€â”€ coco_dataset.json    # Original (before validation)
â”‚       â””â”€â”€ images/              # COCO images with captions aligned
â”œâ”€â”€ robot_selection_data/
â”‚   â””â”€â”€ multi_robot_selection_dataset.json
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ stage1/  # Stage 1 checkpoints
â”‚   â””â”€â”€ stage2/  # Stage 2 checkpoints
â””â”€â”€ src/
    â”œâ”€â”€ train_stage1_vision_language.py
    â”œâ”€â”€ train_stage2_reasoning.py
    â”œâ”€â”€ larima_memory.py
    â”œâ”€â”€ fiber_fusion.py
    â””â”€â”€ ...
```

### Key Features
- **2-Stage Training**: Separate vision-language from reasoning (stable training)
- **Larima GPM**: Bayesian episodic memory (not simple storage)
- **FIBER Fusion**: Queue-based contrastive learning (4096 negatives)
- **Tiny-R1 Reasoning**: GRPO with reward functions (not supervised)
- **BitNet Ready**: Quantization hooks prepared for 1.58-bit deployment

## ğŸ—ï¸ BitGen 2-Stage Training Architecture

### Stage 1: Vision-Language Pre-training (FIBER + Larima GPM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 1: VISION-LANGUAGE PRE-TRAINING                   â”‚
â”‚                     (COCO Dataset - 307K samples)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Captionsâ”‚         â”‚COCO Images   â”‚         â”‚  WandB Monitoring:   â”‚
â”‚ (Tokenized)  â”‚         â”‚(224x224 RGB) â”‚         â”‚  â€¢ Contrastive Loss  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â€¢ Memory KL Loss    â”‚
       â”‚                         â”‚                 â”‚  â€¢ T2I Accuracy      â”‚
       â–¼                         â–¼                 â”‚  â€¢ I2T Accuracy      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  Target: >40% acc    â”‚
â”‚Token + Pos   â”‚         â”‚ DINOv2-base  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Embeddings   â”‚         â”‚(Trainable)   â”‚
â”‚ [B, L, 256]  â”‚         â”‚ [B, P, 768]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FIBER Cross-Modal Fusion                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ ITC Transforms  â”‚      â”‚ Queue Negatives  â”‚              â”‚
â”‚  â”‚ (Separate)      â”‚      â”‚ 4096 text/image  â”‚              â”‚
â”‚  â”‚ Text â†’ Common   â”‚      â”‚ Temperature: 0.07â”‚              â”‚
â”‚  â”‚ Image â†’ Common  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                               â”‚
â”‚  Contrastive Learning: image â†” text matching                â”‚
â”‚  Loss: (loss_t2i + loss_i2t) / 2                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Larimar GPM (Generative Parametric Memory)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ memory_mean: [1000, 256] (trainable)         â”‚            â”‚
â”‚  â”‚ memory_logvar: [1] (Bayesian variance)       â”‚            â”‚
â”‚  â”‚                                               â”‚            â”‚
â”‚  â”‚ Read: Top-K similarity retrieval (k=5)       â”‚            â”‚
â”‚  â”‚ Write: Direct write with importance weights  â”‚            â”‚
â”‚  â”‚ Loss: KL divergence (prior â†” posterior)     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         6-Layer Transformer Attention                         â”‚
â”‚         (Multi-head, batch_first=True)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Checkpoint Saved â”‚
        â”‚ stage1_best.pt   â”‚
        â”‚ To HuggingFace:  â”‚
        â”‚ Every 2 epochs   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 2: Reasoning Module Training (Tiny-R1 + Robot Selection)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 2: REASONING MODULE TRAINING                      â”‚
â”‚                  (Robot Dataset - ~1K samples)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Stage 1 Checkpoint (FROZEN - No gradient updates)                  â”‚
â”‚ âœ“ Vision-Language base with trained representations                     â”‚
â”‚ âœ“ FIBER fusion weights locked                                           â”‚
â”‚ âœ“ Larima GPM memory frozen                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Tiny-R1 Reasoning Module (TRAINABLE)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Reasoning Encoder: [256 â†’ 64]                â”‚            â”‚
â”‚  â”‚ LSTM Processor: max 8 steps                  â”‚            â”‚
â”‚  â”‚ Gate Mechanism: continue/stop decision       â”‚            â”‚
â”‚  â”‚ Reasoning Decoder: [64 â†’ 256]                â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Robot Selector (TRAINABLE - Multi-label)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 5 Robot Types:                                â”‚            â”‚
â”‚  â”‚  1. Drone                                     â”‚            â”‚
â”‚  â”‚  2. Underwater Robot                          â”‚            â”‚
â”‚  â”‚  3. Humanoid                                  â”‚            â”‚
â”‚  â”‚  4. Robot with Wheels                         â”‚            â”‚
â”‚  â”‚  5. Robot with Legs                           â”‚            â”‚
â”‚  â”‚                                               â”‚            â”‚
â”‚  â”‚ Top-3 Selection (independent probabilities)  â”‚            â”‚
â”‚  â”‚ BCE Loss per robot                            â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GRPO Training (Reward-based RL)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Correctness Reward: Did model select right? â”‚            â”‚
â”‚  â”‚ Reasoning Trace Reward: Is trace valid?     â”‚            â”‚
â”‚  â”‚ Loss: -(correctness_reward + trace_reward)  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                               â”‚
â”‚  WandB Monitoring:                                           â”‚
â”‚  â€¢ Robot Selection Loss                                      â”‚
â”‚  â€¢ Correctness Reward                                        â”‚
â”‚  â€¢ Reasoning Reward                                          â”‚
â”‚  â€¢ Robot Selection Accuracy                                  â”‚
â”‚  â€¢ 5x5 Confusion Matrix                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Checkpoint Saved â”‚
        â”‚ stage2_best.pt   â”‚
        â”‚ To HuggingFace:  â”‚
        â”‚ Every epoch      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š WandB Metrics & Visualizations

### Stage 1 Metrics (babylm-ntust team)
- `stage1/loss/total` - Total training loss
- `stage1/loss/contrastive` - Image-text contrastive loss
- `stage1/loss/memory_kl` - Larima GPM KL divergence
- `stage1/accuracy/text_to_image` - Textâ†’Image retrieval accuracy
- `stage1/accuracy/image_to_text` - Imageâ†’Text retrieval accuracy
- `stage1/accuracy/average` - Average contrastive accuracy
- `stage1/learning_rate` - Current learning rate

**Visualizations:**
- Contrastive similarity matrices (image â†” text matching)
- Larima GPM memory state heatmaps (memory utilization)
- Loss curves over epochs
- Accuracy trends (target: >40%)

### Stage 2 Metrics (babylm-ntust team)
- `stage2/loss/total` - Total training loss
- `stage2/loss/robot_selection` - BCE loss for robot classification
- `stage2/reward/correctness` - GRPO correctness reward
- `stage2/reward/reasoning_trace` - Reasoning quality reward
- `stage2/reward/total` - Combined rewards
- `stage2/accuracy/robot_selection` - Multi-label accuracy
- `stage2/learning_rate` - Current learning rate

**Visualizations:**
- 5x5 Robot confusion matrix (per robot type)
- Robot selection accuracy over epochs
- Reward progression (correctness + trace quality)
- Loss curves and convergence
                                    â”‚
                                    â–¼

OUTPUT LAYER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Layer        â”‚  â”‚ Output      â”‚  â”‚ Generated   â”‚  â”‚     Performance Metrics:        â”‚
â”‚Normalizationâ”‚  â”‚Projection   â”‚  â”‚    Text     â”‚  â”‚ â€¢ Throughput: 2-5 tokens/sec   â”‚
â”‚             â”‚  â”‚(Vocab Size) â”‚  â”‚  (Logits)   â”‚  â”‚ â€¢ Latency: 200-500ms/token     â”‚
â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚ â€¢ Memory: <100MB RAM           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Power: <500mW consumption    â”‚
                                                    â”‚ â€¢ Temperature: <70Â°C           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚Attention    â”‚  â”‚ Robot       â”‚  â”‚  Robot      â”‚
â”‚Cache Update â”‚  â”‚Selection    â”‚  â”‚Probabilitiesâ”‚
â”‚(Next Token) â”‚  â”‚(Confidence) â”‚  â”‚ (16 types)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow Process

### **1. Input Processing**
```
Text Input â†’ Tokenization â†’ Token IDs [batch_size, seq_len]
Image Input â†’ Patch Extraction â†’ RGB Patches [batch_size, 3, 14, 14]
```

### **2. Embedding & Quantization**
```
Token IDs â†’ Token Embedding + Positional Embedding [batch_size, seq_len, embed_dim]
RGB Patches â†’ Vision Encoder â†’ Vision Features [batch_size, 1, embed_dim]
All Weights â†’ BitNet Quantization â†’ {-1, 0, +1} ternary values
```

### **3. Episodic Memory Integration** ğŸ§ 
```
Input Embeddings â†’ Key/Value Projection â†’ Memory Query
Memory Keys (64 slots) â†’ Similarity Computation â†’ Attention Weights
Attention Weights Ã— Memory Values â†’ Retrieved Memories
Input + Retrieved Memories â†’ Enhanced Representation

Edge Operations:
â€¢ Fast Fact Edit: Update memory slot directly (~1-5ms)
â€¢ Selective Forget: Decay specific memory strength 
â€¢ Online Update: Learn from deployment experiences
â€¢ Local Retrieval: Access relevant memories (<1ms)
```

### **4. Cross-Modal Fusion (FIBER)**
```
Text Embeddings + Vision Features â†’ Cross-Attention
Text-to-Vision Attention â†’ Vision-enhanced Text
Vision-to-Text Attention â†’ Text-enhanced Vision  
Concatenate â†’ Fusion MLP â†’ Multimodal Representation
```

### **5. Multi-Layer Attention with Sinks**
```
For each layer (4 layers):
  Input â†’ Q, K, V Projections
  Attention Sinks (4 tokens) + Sliding Window (128 tokens)
  Multi-Head Attention â†’ Attention Weights [batch, heads, seq, seq]
  Attention Weights Ã— Values â†’ Attended Output
  Update Cache for Next Iteration
```

### **6. Reasoning Module (Tiny-R1)**
```
Attended Features â†’ Reasoning Encoder â†’ Reasoning Space
For each reasoning step (max 8 steps):
  LSTM Processing â†’ Reasoning State
  Gate Network â†’ Continue/Stop Decision
  Accumulate Reasoning States
Final Reasoning â†’ Decoder â†’ Enhanced Features
```

### **7. Robot Selection (Top-3 Multi-Label with Chain-of-Thought)**
```
Task/Scene Representation â†’ Tiny-R1 Reasoning (3-8 steps)
Reasoning Output â†’ Task Encoder â†’ Task Features
For each robot (5 types: Drone, Underwater, Humanoid, Wheels, Legs):
  Task Features + Robot Embedding â†’ Binary Score (independent)
  Sigmoid Activation â†’ Robot Suitability Probability [0, 1]
All Robot Probabilities â†’ Top-K Selection (k=3)
Top-3 Robots + Confidences â†’ Multi-Robot Deployment Decision
Ground Truth vs Predictions â†’ 5x5 Confusion Matrix Update
```

### **8. Output Generation**
```
Enhanced Features â†’ Layer Normalization
Normalized Features â†’ Output Projection â†’ Logits [batch_size, seq_len, vocab_size]
Logits â†’ Text Generation (sampling/greedy)
Robot Probabilities â†’ Robot Selection Output
Attention States â†’ Cache for Next Token
```

## ğŸ§  Episodic Memory: The Key Advantage

BitGen's episodic memory system provides **critical advantages for edge deployment**:

### ğŸš€ **Fast Local Knowledge Access**
- **Low Latency**: Memory accessed locally on device, eliminating network delays
- **Edge-Optimized**: Knowledge retrieval happens on-device without cloud dependency
- **Real-time Updates**: Immediate access to latest information and experiences

### âš¡ **Dynamic Knowledge Management** (No Retraining Required)
- **Fast Fact Editing**: Update knowledge instantly without model retraining
- **Selective Forgetting**: Remove outdated information while preserving important memories
- **High Accuracy on Updated Knowledge**: Maintains performance on new information
- **Continuous Learning**: Adapts to new experiences during deployment

### ğŸ¯ **Latent Information Advantages**
- **Compressed Knowledge**: Stores experiences as latent representations for efficiency
- **Contextual Retrieval**: Accesses relevant memories based on current context
- **Memory Efficiency**: Compact storage suitable for edge device constraints
- **Experience-Based Learning**: Learns from actual deployment experiences

**Unlike traditional LLMs that require full retraining for knowledge updates, BitGen's episodic memory enables real-time knowledge management directly on edge devices.**

## âœ… All Features & Modules Present

### Core Model Components (bitgen_model.py)
- âœ… **BitGenConfig** - Model configuration with all hyperparameters
- âœ… **BitNetLinear** - 1.58-bit quantization layer ({-1, 0, +1} weights)
- âœ… **EpisodicMemory** - Original episodic memory (64 slots, key-value)
- âœ… **VisionAttentionSink** - Vision-specific attention with sinks
- âœ… **AttentionSink** - Text attention with sliding window (4 sinks, 128 window)
- âœ… **CrossModalFusion** - DINOv2-based cross-modal fusion (legacy, kept for compatibility)
- âœ… **ReasoningModule** - Tiny-R1 style reasoning (LSTM, gate mechanism, 8 steps)
- âœ… **RobotSelector** - Multi-label robot selection (5 types, top-3 selection)
- âœ… **BitGenModel** - Complete integrated model (for inference/deployment)

### New 2-Stage Training Components
- âœ… **larima_memory.py** - Larima GPM with Bayesian inference (1000 slots)
- âœ… **fiber_fusion.py** - FIBER cross-modal with queue-based contrastive (4096 queues)
- âœ… **train_stage1_vision_language.py** - Stage 1 training script
- âœ… **train_stage2_reasoning.py** - Stage 2 training script

### Data Loaders (data_loader.py)
- âœ… **COCODataset** - COCO image-caption pairs (307K samples)
- âœ… **RobotDataset** - Robot selection with multi-label classification
- âœ… **BitGenTokenizer** - Embedded-optimized tokenizer (8192 vocab)

### Monitoring & Integration
- âœ… **wandb_integration.py** - WandB logging (babylm-ntust team)
  - Stage 1/2 specific metrics
  - Contrastive similarity visualizations
  - Robot confusion matrix
  - Larima GPM memory heatmaps
- âœ… **huggingface_integration.py** - Auto-push to HuggingFace Hub
  - Stage 1: `{username}/BitGen-Reasoning-stage1`
  - Stage 2: `{username}/BitGen-Reasoning-stage2`

### All Original Features Preserved
- âœ… **Attention Sinks** - 4 sink tokens + 128 sliding window
- âœ… **BitNet Quantization** - Ready for 1.58-bit deployment
- âœ… **Episodic Memory** - Both original (64 slots) and Larima GPM (1000 slots)
- âœ… **FIBER Fusion** - Queue-based contrastive learning
- âœ… **Tiny-R1 Reasoning** - GRPO training with rewards
- âœ… **Robot Selection** - Top-3 multi-label classification
- âœ… **DINOv2 Vision** - Trainable end-to-end vision encoder

## ï¿½ Quick Start: Complete Setup (Sequential Steps)

### Step 1: Clone and Install Dependencies
```bash
git clone https://github.com/euhidaman/BitGen.git
cd BitGen
pip install -r requirements.txt
```

### Step 2: Setup Kaggle API for Dataset Download
```bash
# Place your kaggle.json in ~/.kaggle/ directory
# Get kaggle.json from: https://www.kaggle.com/settings/account
# Linux/Mac:
mkdir -p ~/.kaggle
mv kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Windows (PowerShell):
mkdir $env:USERPROFILE\.kaggle -Force
Move-Item kaggle.json $env:USERPROFILE\.kaggle\
```

### Step 3: Download COCO Dataset from Kaggle
```bash
python download_coco_dataset.py
# This downloads COCO 2017 validation set to data/coco/
```

### Step 4: Download Robot Selection Dataset
```bash
# Download multi_robot_selection_dataset.json from your data source
# Place it in: robot_selection_data/data/Multi-Robot-Selection/
mkdir -p ../robot_selection_data/data/Multi-Robot-Selection
# Copy your multi_robot_selection_dataset.json to this directory
```

### Step 5: Setup HuggingFace Hub (Optional - for model pushing)
```bash
# PowerShell:
$env:HF_TOKEN="your_huggingface_token"

# OR login via CLI:
huggingface-cli login
```

### Step 6: Setup WandB (Optional - for metrics tracking)
```bash
wandb login
# Make sure you have access to 'babylm-ntust' team
```

### Step 7: Train the Model
```bash
python bitgen_cli.py train `
  --coco_data data/coco `
  --robot_data ../robot_selection_data/data `
  --model_size tiny `
  --batch_size 16 `
  --num_epochs 10 `
  --use_wandb
```

## ğŸ“ Training

### Basic Training with All Features
```bash
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --model_size tiny \
  --num_epochs 10 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb
```

### Advanced Training with Custom Settings
```bash
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --robot_data data/robot_selection/robot_tasks.json \
  --model_size tiny \
  --batch_size 8 \
  --learning_rate 1e-4 \
  --num_epochs 15 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --hf_repo_name "my-bitgen-model" \
  --use_wandb \
  --wandb_entity babylm-ntust \
  --wandb_project bitgen-training \
  --wandb_tags bitgen multimodal robotics
```

**What Training Provides:**
- âœ… **FLOPS Tracking**: Real-time computational complexity monitoring
- âœ… **CodeCarbon Energy Monitoring**: Energy consumption and carbon footprint tracking
- âœ… **HuggingFace Hub Pushing**: Automatic model upload after every epoch
- âœ… **WandB Logging**: Comprehensive metrics to babylm-ntust team
- âœ… **Advanced Visualizations**: Memory heatmaps, attention patterns, reasoning matrices

## ğŸ¯ Deployment Strategy

**BitGen is designed for this exact workflow:**

### ğŸš€ Training Environment (RTX 4090)
- **High-performance training** with full GPU acceleration
- **Comprehensive monitoring** with FLOPS, energy tracking, and visualizations
- **Advanced metrics** including episodic memory heatmaps and reasoning matrices
- **Automatic model pushing** to HuggingFace Hub after every epoch

### ğŸ“± Inference Environment (Raspberry Pi Zero)
- **Optimized inference** with 1.58-bit quantization for ultra-low power
- **Edge monitoring** with thermal, power, and performance tracking
- **Fast episodic memory operations** (fact editing, selective forgetting)
- **Local knowledge management** without cloud dependencies

## ğŸ”§ Platform-Specific Optimizations

### For Training (RTX 4090):
```bash
# Full-featured training with all monitoring
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --model_size tiny \
  --batch_size 32 \
  --num_epochs 50 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb
```

### For Inference (Raspberry Pi Zero):
```bash
# Optimized inference with Pi-specific monitoring
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_final.pt \
  --benchmark \
  --show_metrics
```

## ğŸ”® Inference with Performance Metrics

### Interactive Inference with Real-time Monitoring
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --interactive
```

### Comprehensive Benchmark
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --benchmark \
  --num_samples 20 \
  --show_metrics
```

### Single Inference with Metrics
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt
```

**Inference Metrics Provided:**
- ğŸ¯ **Model Response Throughput**: tokens/sec
- â±ï¸ **Latency**: ms per token and per response
- ğŸ’¾ **Memory Footprint**: RAM usage and peak memory
- âš¡ **Power Consumption**: mW power usage
- ğŸŒ¡ï¸ **Thermal Profile**: CPU temperature monitoring

### Example Inference Output
```
ğŸ“Š COMPREHENSIVE INFERENCE METRICS:
ğŸ¯ PERFORMANCE:
   Model Response Throughput: 3.45 tokens/sec
   Latency per Token: 289.2 ms/token
   Response Time: 1247.8 ms

ğŸ’¾ MEMORY FOOTPRINT:
   Peak RAM Usage: 87.3 MB
   Memory Delta: +12.4 MB

âš¡ POWER & ENERGY:
   Power Consumption: 387.2 mW
   Energy Consumed: 12.4 mJ

ğŸŒ¡ï¸ THERMAL PROFILE:
   CPU Temperature: 62.4Â°C
   Thermal Delta: +2.1Â°C
```

## ğŸ“ˆ Monitoring & Analysis

### System Monitoring
```bash
python bitgen_cli.py monitor --duration 300 --real_time
```

### Results Analysis
```bash
python bitgen_cli.py analyze --results_dir training_monitoring --generate_report
```

## ğŸ—ï¸ Model Architecture

- **Embed Dimensions**: 128D (tiny), 256D (small), 64D (nano)
- **Layers**: 4 layers (tiny), 6 layers (small), 2 layers (nano)
- **Episodic Memory**: 64 memory slots with retrieval and update mechanisms
- **Attention Sinks**: 4 sink tokens for efficient long-sequence processing
- **Cross-Modal Fusion**: Text-image understanding with FIBER architecture
- **Tiny-R1 Reasoning**: Chain-of-thought reasoning with 3-8 LSTM steps
- **Robot Selection**: 5 robot types (Drone, Underwater Robot, Humanoid, Robot with Wheels, Robot with Legs)
  - Top-3 multi-label classification with sigmoid activation
  - 5x5 confusion matrix tracking per epoch
  - Interleaved training: 90% COCO vision-language, 10% robot selection
- **Quantization**: 1.58-bit weights for deployment efficiency

## ğŸ¯ Use Cases

### 1. Multimodal Image Captioning
```python
from src import BitGen

bitgen = BitGen(model_size='tiny')
bitgen.load_checkpoint('checkpoints/best.pt')

# Process image with text
result = bitgen.process_image_and_text('image.jpg', 'Describe the scene')
```

### 2. Robot Task Selection (Top-3 Multi-Label)
```python
# Select top-3 most suitable robots for task/scene
robot_selection = bitgen.select_robots_for_task(
    task_description='Navigate rocky terrain to deliver medical supplies',
    image_path='scene.jpg'  # Optional: scene image for visual context
)

print("ğŸ¤– Top-3 Robot Recommendations:")
for i, (robot, confidence) in enumerate(zip(robot_selection['top_k_robots'], robot_selection['top_k_probs'])):
    print(f"  {i+1}. {robot:<25} (confidence: {confidence:.3f})")

# Output:
# ğŸ¤– Top-3 Robot Recommendations:
#   1. Robot with Legs         (confidence: 0.892)
#   2. Drone                   (confidence: 0.745)
#   3. Robot with Wheels       (confidence: 0.623)
```

### 3. Text Generation with Reasoning
```python
# Generate text with reasoning
response = bitgen.generate_text('<reasoning>The robot should move to</reasoning><answer>')
```

## ğŸ“Š WandB Dashboard (babylm-ntust team)

Training automatically logs to WandB with:
- **Loss curves** and training metrics
- **Episodic memory heatmaps** showing memory utilization
- **Attention pattern visualizations** highlighting important tokens
- **Robot selection matrices** showing reasoning improvement
- **Performance dashboards** with throughput, latency, power consumption
- **Energy tracking** with FLOPS and carbon footprint analysis

Access your runs at: `https://wandb.ai/babylm-ntust/bitgen-training`

## ğŸ¤— HuggingFace Hub Integration

Models are automatically pushed to HuggingFace Hub after every epoch:
- **Auto-generated model names**: `bitgen-{size}-{timestamp}`
- **Detailed model cards** with training metrics and usage instructions
- **Version tracking** with epoch-specific commits
- **Public sharing** for easy collaboration

## ğŸ”§ Configuration Options

### Model Sizes
- **nano**: 64D embed, 2 layers (ultra-lightweight)
- **tiny**: 128D embed, 4 layers (default)
- **small**: 256D embed, 6 layers (higher capacity)

### CLI Commands
- `download`: Download and prepare COCO dataset
- `train`: Train model with comprehensive monitoring
- `inference`: Run inference with performance metrics
- `evaluate`: Evaluate model capabilities
- `deploy`: Deploy for embedded systems
- `monitor`: System performance monitoring
- `analyze`: Analyze training/inference results

## ğŸ›ï¸ BitGen Architecture Overview

### **Encoder-Decoder Design (BitMar-Inspired)**

BitGen follows an **encoder-decoder architecture** similar to BitMar, combining the best elements from three research projects:

1. **BitMar**: Encoder-decoder structure with multi-component loss
2. **FIBER**: Cross-modal fusion with queue-based contrastive learning
3. **Larimar**: Enhanced Generative Parametric Memory (GPM) with Bayesian inference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BITGEN ARCHITECTURE                              â”‚
â”‚                  (BitMar + FIBER + Enhanced Larimar)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT STAGE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Tokens  â”‚         â”‚ Image Patchesâ”‚
â”‚ [B, seq_len] â”‚         â”‚ [B, patches] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Token + Pos   â”‚         â”‚DINOv2 Vision â”‚
â”‚Embeddings    â”‚         â”‚Encoder       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FIBER CROSS-MODAL FUSION                      â”‚
â”‚  â€¢ Queue-based contrastive learning (4096 negative samples)    â”‚
â”‚  â€¢ Temperature-scaled similarity (Ï„ = 0.1)                      â”‚
â”‚  â€¢ Bidirectional alignment: image â†” text                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ENHANCED LARIMAR GPM MEMORY                        â”‚
â”‚  â€¢ Bayesian parametric memory: mean + logvar                    â”‚
â”‚  â€¢ Top-K retrieval with cosine similarity (k=5)                 â”‚
â”‚  â€¢ Memory quality tracking (read/write counts)                  â”‚
â”‚  â€¢ KL divergence regularization                                 â”‚
â”‚  â€¢ Save/load capability for external storage                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MULTI-LAYER ATTENTION (6 layers)                      â”‚
â”‚  â€¢ Attention Sinks for streaming inference                      â”‚
â”‚  â€¢ Multi-head self-attention                                    â”‚
â”‚  â€¢ Gradient monitoring for stability                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              REASONING MODULE (Tiny-R1 Style)                   â”‚
â”‚  â€¢ LSTM-based chain-of-thought reasoning                        â”‚
â”‚  â€¢ Adaptive reasoning steps (max 8)                             â”‚
â”‚  â€¢ Gate mechanism for reasoning depth                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼                â–¼                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ENCODER OUTPUT â”‚ â”‚  DECODER   â”‚ â”‚ROBOT SELECTORâ”‚
        â”‚   (Direct)     â”‚ â”‚   (New!)   â”‚ â”‚  (Top-K)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DECODER STAGE (BitMar-Style):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BITNET TEXT DECODER                             â”‚
â”‚  â€¢ Multi-layer decoder with causal attention                    â”‚
â”‚  â€¢ Cross-attention to encoder output                            â”‚
â”‚  â€¢ Teacher forcing during training                              â”‚
â”‚  â€¢ Autoregressive generation during inference                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚Text Reconstructionâ”‚
        â”‚    Logits        â”‚
        â”‚ [B, tgt_len, V]  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Multi-Component Loss Function (BitMar-Style)**

The model is trained with three loss components, following BitMar's approach:

```
Total Loss = Î±Â·Text Loss + Î²Â·Contrastive Loss + Î³Â·Memory KL Loss

Where:
  Î± = 1.0   (text reconstruction - main learning signal)
  Î² = 0.1   (image-text alignment - FIBER-style)
  Î³ = 0.05  (memory regularization - Larimar-style)
```

**Loss Components:**

1. **Text Reconstruction Loss** (Î± = 1.0) - PRIMARY SIGNAL
   - Cross-entropy on decoder output
   - Next-token prediction with teacher forcing
   - Metrics: Perplexity, Token Accuracy
   - This is the main learning objective

2. **Contrastive Loss** (Î² = 0.1) - ALIGNMENT
   - FIBER-style queue-based contrastive learning
   - Symmetric: loss_t2i + loss_i2t
   - Temperature-scaled similarity (Ï„ = 0.1)
   - Ensures vision-language alignment

3. **Memory KL Divergence** (Î³ = 0.05) - REGULARIZATION
   - Larimar-style Bayesian memory regularization
   - KL(posterior || prior) on memory distributions
   - Prevents memory overfitting
   - Encourages generalization

### **Key Improvements from Original BitGen**

| Component           | Before                  | After (Current)                                         |
| ------------------- | ----------------------- | ------------------------------------------------------- |
| **Architecture**    | Encoder only            | Encoder-decoder (BitMar-style)                          |
| **Loss Function**   | Contrastive only        | Multi-component (text + contrastive + memory KL)        |
| **Memory**          | Simple GPM              | Enhanced GPM with Bayesian inference + quality tracking |
| **Cross-Modal**     | Basic fusion            | FIBER-style queue-based (4096 negatives)                |
| **Text Generation** | Direct projection       | Proper decoder with causal attention                    |
| **Learning Signal** | Weak (contrastive only) | Strong (text reconstruction primary)                    |
| **Optimization**    | Fixed LR                | Warmup + adaptive (ReduceLROnPlateau)                   |

### **Training Stability Features**

- **Warmup Scheduler**: 1000 steps linear warmup
- **Adaptive LR**: ReduceLROnPlateau (patience=3, factor=0.5)
- **Temperature**: 0.1 (smoother gradients than 0.07)
- **Gradient Monitoring**: Track gradient norms every 100 steps
- **Mixed Precision**: torch.amp.autocast for faster training
- **Gradient Clipping**: Prevents exploding gradients

### **Architecture Diagram: Loss Computation Flow**

```
Forward Pass â†’ Model Outputs â†’ Compute Multi-Loss â†’ Backprop
     â”‚              â”‚                    â”‚                â”‚
     â”‚              â”œâ”€â”€ decoder_logits â”€â”€â”¤                â”‚
     â”‚              â”œâ”€â”€ contrastive_featâ”€â”¤                â”‚
     â”‚              â””â”€â”€ memory_kl â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
     â”‚                                    â”‚                â”‚
     â–¼                                    â–¼                â–¼
 [Input]                        [Loss Components]    [Gradients]
  Text                           â€¢ text_loss (1.0)      â”‚
  Images                         â€¢ contrastive (0.1)    â”‚
  Targets                        â€¢ memory_kl (0.05)     â”‚
                                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                                 Total Loss = Î£(Î±áµ¢Â·Láµ¢)  â”‚
                                                        â–¼
                                                   [Optimizer]
```

## ğŸ“ˆ Performance Targets

### Training Efficiency
- **FLOPS Tracking**: Real-time computational complexity monitoring
- **Energy Monitoring**: CodeCarbon integration for sustainability
- **Memory Optimization**: Episodic memory utilization tracking

### Inference Performance
- **Throughput**: 2-5 tokens/sec on Raspberry Pi
- **Latency**: 200-500ms per token
- **Memory**: <100MB RAM usage
- **Power**: <500mW consumption

## ğŸ¨ Visualizations Available

1. **Memory Access Heatmaps**: Show episodic memory usage patterns
2. **Attention Focus Maps**: Highlight most important tokens per head
3. **Robot Selection Matrices**: Track reasoning accuracy improvement
4. **Performance Dashboards**: Real-time metrics and trends
5. **Energy Efficiency Charts**: FLOPS per mJ, carbon per token

## ğŸ” Troubleshooting

### Common Issues
- **HuggingFace Login**: Ensure `HF_TOKEN` environment variable is set
- **WandB Access**: Verify access to 'babylm-ntust' team
- **Memory Issues**: Reduce batch size or model size for resource-constrained systems
- **Temperature Warnings**: Training automatically pauses if CPU temperature >75Â°C

### Monitoring
All training runs include automatic monitoring and will generate:
- Comprehensive training reports with FLOPS and energy data
- Advanced metrics visualizations saved locally and to WandB
- Model checkpoints with full state information

## ğŸ“ Example Complete Workflow

```bash
# 1. Download data
python bitgen_cli.py download

# 2. Train with full monitoring
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb

# 3. Run inference with metrics
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --benchmark \
  --show_metrics

# 4. Analyze results
python bitgen_cli.py analyze \
  --results_dir training_monitoring \
  --generate_report
```

The BitGen system provides state-of-the-art multimodal capabilities with comprehensive monitoring, automatic model sharing, and advanced internal analysis - all optimized for efficient deployment and team collaboration.
