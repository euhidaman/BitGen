# BitGen: Advanced Tiny Language Model for Embedded Systems

An advanced tiny language model that integrates **Larimar Episodic Memory**, **BitNet 1.58-bit Quantization**, **FIBER Cross-Modal Fusion**, **Attention Sinks**, **Tiny-R1 Reasoning**, and **Robot Selection** capabilities.

## ğŸ—ï¸ BitGen Architecture Block Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BitGen Complete Architecture Flow                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT LAYER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Input  â”‚    â”‚ Image Input â”‚    â”‚         Monitoring & Integration          â”‚
â”‚ (Token IDs) â”‚    â”‚ (RGB Tensor)â”‚    â”‚ â€¢ WandB (babylm-ntust team)             â”‚
â”‚             â”‚    â”‚             â”‚    â”‚ â€¢ HuggingFace Hub (auto-push)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ FLOPS Tracking                        â”‚
       â”‚                   â”‚          â”‚ â€¢ CodeCarbon Energy Monitoring          â”‚
       â–¼                   â–¼          â”‚ â€¢ Performance Profiling                  â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EMBEDDING LAYER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Token Embed  â”‚    â”‚Vision Encodeâ”‚    â”‚         BitNet 1.58-bit Quantization     â”‚
â”‚+ Positional â”‚    â”‚(DinoV2-like)â”‚    â”‚ â€¢ Weights: {-1, 0, +1}                  â”‚
â”‚Encoding     â”‚    â”‚14x14 Patchesâ”‚    â”‚ â€¢ Activations: 8-bit                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ 4x Compression Ratio                  â”‚
       â”‚                   â”‚          â”‚ â€¢ Integer Arithmetic for Edge           â”‚
       â–¼                   â–¼          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â–¼

CORE PROCESSING LAYERS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Larimar Episodic Memory (Key Component)                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Memory Keys  â”‚  â”‚Memory Valuesâ”‚  â”‚              Edge Advantages:                     â”‚  â”‚
â”‚ â”‚(64 slots)   â”‚  â”‚(64 slots)   â”‚  â”‚ âš¡ Fast Fact Editing (no retraining) ~1-5ms      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ ğŸ—‘ï¸ Selective Forgetting of outdated info       â”‚  â”‚
â”‚        â”‚               â”‚          â”‚ ğŸ“ˆ High Accuracy on Updated Knowledge           â”‚  â”‚
â”‚        â–¼               â–¼          â”‚ ğŸš€ Local Access (no cloud dependency)           â”‚  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ğŸ’¾ Latent Information Storage                  â”‚  â”‚
â”‚ â”‚    Similarity Computation       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”‚    & Attention Weights          â”‚                                                      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            FIBER Cross-Modal Fusion                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Text Featuresâ”‚â—„â”€â”¤Text-Vision  â”‚  â”‚                Fusion Process:                  â”‚  â”‚
â”‚ â”‚             â”‚  â”‚ Attention   â”‚  â”‚ 1. Text-to-Vision Cross Attention              â”‚  â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚ 2. Vision-to-Text Cross Attention              â”‚  â”‚
â”‚ â”‚Vision Feats â”‚â—„â”€â”¤Vision-Text  â”‚  â”‚ 3. Multimodal Representation Creation          â”‚  â”‚
â”‚ â”‚             â”‚  â”‚ Attention   â”‚  â”‚ 4. Joint Feature Space Mapping                 â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Layer Attention with Sinks                                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Attention    â”‚  â”‚ Sink Tokens â”‚  â”‚             Attention Features:                 â”‚  â”‚
â”‚ â”‚Head 1       â”‚  â”‚   (4 slots) â”‚  â”‚ â€¢ 4 Attention Sink Tokens                      â”‚  â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚ â€¢ 128 Token Sliding Window                     â”‚  â”‚
â”‚ â”‚Attention    â”‚  â”‚Recent Windowâ”‚  â”‚ â€¢ Memory-Efficient Long Sequences              â”‚  â”‚
â”‚ â”‚Head 2-8     â”‚  â”‚ (128 tokens)â”‚  â”‚ â€¢ Multi-Head Specialization Analysis           â”‚  â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚ â€¢ Important Token Identification               â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Tiny-R1 Reasoning Module                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Reasoning    â”‚  â”‚  LSTM Step  â”‚  â”‚            Reasoning Process:                   â”‚  â”‚
â”‚ â”‚Encoder      â”‚  â”‚ Processor   â”‚  â”‚ 1. Encode input to reasoning space             â”‚  â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚ 2. Multi-step LSTM processing                  â”‚  â”‚
â”‚ â”‚Gate         â”‚  â”‚ Reasoning   â”‚  â”‚ 3. Gate mechanism (continue/stop)              â”‚  â”‚
â”‚ â”‚Mechanism    â”‚  â”‚ Decoder     â”‚  â”‚ 4. Aggregate reasoning steps                   â”‚  â”‚
â”‚ â”‚             â”‚  â”‚             â”‚  â”‚ 5. Decode back to embedding space              â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Robot Selection System (Top-3 Multi-Label)                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚Task Encoder â”‚  â”‚Robot        â”‚  â”‚           Selection Process:                    â”‚  â”‚
â”‚ â”‚             â”‚  â”‚Embeddings   â”‚  â”‚ 1. Encode task/scene representation            â”‚  â”‚
â”‚ â”‚Tiny-R1      â”‚  â”‚(5 robots)   â”‚  â”‚ 2. Chain-of-thought reasoning (3-8 steps)     â”‚  â”‚
â”‚ â”‚Reasoning    â”‚  â”‚             â”‚  â”‚ 3. Multi-label classification (sigmoid)        â”‚  â”‚
â”‚ â”‚             â”‚  â”‚Top-3        â”‚  â”‚ 4. Select top-3 most suitable robots          â”‚  â”‚
â”‚ â”‚Confusion    â”‚  â”‚Selection    â”‚  â”‚ 5. Generate 5x5 confusion matrix              â”‚  â”‚
â”‚ â”‚Matrix 5x5   â”‚  â”‚Network      â”‚  â”‚ 6. Track accuracy improvement per epoch       â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                         â”‚
â”‚  Robots: Drone | Underwater Robot | Humanoid | Robot with Wheels | Robot with Legs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼

OUTPUT LAYER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Layer        â”‚  â”‚ Output      â”‚  â”‚ Generated   â”‚  â”‚     Performance Metrics:        â”‚
â”‚Normalizationâ”‚  â”‚Projection   â”‚  â”‚    Text     â”‚  â”‚ â€¢ Throughput: 2-5 tokens/sec   â”‚
â”‚             â”‚  â”‚(Vocab Size) â”‚  â”‚  (Logits)   â”‚  â”‚ â€¢ Latency: 200-500ms/token     â”‚
â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚ â€¢ Memory: <100MB RAM           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Power: <500mW consumption    â”‚
                                                    â”‚ â€¢ Temperature: <70Â°C           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚Attention    â”‚  â”‚ Robot       â”‚  â”‚  Robot      â”‚
â”‚Cache Update â”‚  â”‚Selection    â”‚  â”‚Probabilitiesâ”‚
â”‚(Next Token) â”‚  â”‚(Confidence) â”‚  â”‚ (16 types)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow Process

### **1. Input Processing**
```
Text Input â†’ Tokenization â†’ Token IDs [batch_size, seq_len]
Image Input â†’ Patch Extraction â†’ RGB Patches [batch_size, 3, 14, 14]
```

### **2. Embedding & Quantization**
```
Token IDs â†’ Token Embedding + Positional Embedding [batch_size, seq_len, embed_dim]
RGB Patches â†’ Vision Encoder â†’ Vision Features [batch_size, 1, embed_dim]
All Weights â†’ BitNet Quantization â†’ {-1, 0, +1} ternary values
```

### **3. Episodic Memory Integration** ğŸ§ 
```
Input Embeddings â†’ Key/Value Projection â†’ Memory Query
Memory Keys (64 slots) â†’ Similarity Computation â†’ Attention Weights
Attention Weights Ã— Memory Values â†’ Retrieved Memories
Input + Retrieved Memories â†’ Enhanced Representation

Edge Operations:
â€¢ Fast Fact Edit: Update memory slot directly (~1-5ms)
â€¢ Selective Forget: Decay specific memory strength 
â€¢ Online Update: Learn from deployment experiences
â€¢ Local Retrieval: Access relevant memories (<1ms)
```

### **4. Cross-Modal Fusion (FIBER)**
```
Text Embeddings + Vision Features â†’ Cross-Attention
Text-to-Vision Attention â†’ Vision-enhanced Text
Vision-to-Text Attention â†’ Text-enhanced Vision  
Concatenate â†’ Fusion MLP â†’ Multimodal Representation
```

### **5. Multi-Layer Attention with Sinks**
```
For each layer (4 layers):
  Input â†’ Q, K, V Projections
  Attention Sinks (4 tokens) + Sliding Window (128 tokens)
  Multi-Head Attention â†’ Attention Weights [batch, heads, seq, seq]
  Attention Weights Ã— Values â†’ Attended Output
  Update Cache for Next Iteration
```

### **6. Reasoning Module (Tiny-R1)**
```
Attended Features â†’ Reasoning Encoder â†’ Reasoning Space
For each reasoning step (max 8 steps):
  LSTM Processing â†’ Reasoning State
  Gate Network â†’ Continue/Stop Decision
  Accumulate Reasoning States
Final Reasoning â†’ Decoder â†’ Enhanced Features
```

### **7. Robot Selection (Top-3 Multi-Label with Chain-of-Thought)**
```
Task/Scene Representation â†’ Tiny-R1 Reasoning (3-8 steps)
Reasoning Output â†’ Task Encoder â†’ Task Features
For each robot (5 types: Drone, Underwater, Humanoid, Wheels, Legs):
  Task Features + Robot Embedding â†’ Binary Score (independent)
  Sigmoid Activation â†’ Robot Suitability Probability [0, 1]
All Robot Probabilities â†’ Top-K Selection (k=3)
Top-3 Robots + Confidences â†’ Multi-Robot Deployment Decision
Ground Truth vs Predictions â†’ 5x5 Confusion Matrix Update
```

### **8. Output Generation**
```
Enhanced Features â†’ Layer Normalization
Normalized Features â†’ Output Projection â†’ Logits [batch_size, seq_len, vocab_size]
Logits â†’ Text Generation (sampling/greedy)
Robot Probabilities â†’ Robot Selection Output
Attention States â†’ Cache for Next Token
```

## ğŸ§  Episodic Memory: The Key Advantage

BitGen's episodic memory system provides **critical advantages for edge deployment**:

### ğŸš€ **Fast Local Knowledge Access**
- **Low Latency**: Memory accessed locally on device, eliminating network delays
- **Edge-Optimized**: Knowledge retrieval happens on-device without cloud dependency
- **Real-time Updates**: Immediate access to latest information and experiences

### âš¡ **Dynamic Knowledge Management** (No Retraining Required)
- **Fast Fact Editing**: Update knowledge instantly without model retraining
- **Selective Forgetting**: Remove outdated information while preserving important memories
- **High Accuracy on Updated Knowledge**: Maintains performance on new information
- **Continuous Learning**: Adapts to new experiences during deployment

### ğŸ¯ **Latent Information Advantages**
- **Compressed Knowledge**: Stores experiences as latent representations for efficiency
- **Contextual Retrieval**: Accesses relevant memories based on current context
- **Memory Efficiency**: Compact storage suitable for edge device constraints
- **Experience-Based Learning**: Learns from actual deployment experiences

**Unlike traditional LLMs that require full retraining for knowledge updates, BitGen's episodic memory enables real-time knowledge management directly on edge devices.**

## ğŸ“Š Advanced Metrics & Visualizations

### Episodic Memory Analysis
- Memory utilization and diversity tracking
- Access pattern heatmaps showing which memory slots are used
- Memory similarity matrices revealing relationships between stored experiences

### Attention Heatmaps
- Multi-head attention visualization focusing on important tokens
- Head specialization analysis (local vs global attention patterns)
- Attention sink detection and important token identification

### Reasoning Matrices
- **5x5 Robot Confusion Matrix**: Tracks prediction accuracy for 5 robot types
  - Rows: True robots (Drone, Underwater Robot, Humanoid, Robot with Wheels, Robot with Legs)
  - Columns: Predicted robots (top-3 selections per sample)
  - Updated every epoch with normalized frequencies
- **Per-Robot Accuracy**: Individual robot selection improvement over epochs
- **Chain-of-Thought Traces**: Logged reasoning steps showing multi-step decision process
- **Interactive Dashboards**: Real-time accuracy trends and confusion patterns

## ï¿½ Quick Start: Complete Setup (Sequential Steps)

### Step 1: Clone and Install Dependencies
```bash
git clone https://github.com/euhidaman/BitGen.git
cd BitGen
pip install -r requirements.txt
```

### Step 2: Setup Kaggle API for Dataset Download
```bash
# Place your kaggle.json in ~/.kaggle/ directory
# Get kaggle.json from: https://www.kaggle.com/settings/account
# Linux/Mac:
mkdir -p ~/.kaggle
mv kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Windows (PowerShell):
mkdir $env:USERPROFILE\.kaggle -Force
Move-Item kaggle.json $env:USERPROFILE\.kaggle\
```

### Step 3: Download COCO Dataset from Kaggle
```bash
python download_coco_dataset.py
# This downloads COCO 2017 validation set to data/coco/
```

### Step 4: Download Robot Selection Dataset
```bash
# Download multi_robot_selection_dataset.json from your data source
# Place it in: robot_selection_data/data/Multi-Robot-Selection/
mkdir -p ../robot_selection_data/data/Multi-Robot-Selection
# Copy your multi_robot_selection_dataset.json to this directory
```

### Step 5: Setup HuggingFace Hub (Optional - for model pushing)
```bash
# PowerShell:
$env:HF_TOKEN="your_huggingface_token"

# OR login via CLI:
huggingface-cli login
```

### Step 6: Setup WandB (Optional - for metrics tracking)
```bash
wandb login
# Make sure you have access to 'babylm-ntust' team
```

### Step 7: Train the Model
```bash
python bitgen_cli.py train `
  --coco_data data/coco `
  --robot_data ../robot_selection_data/data `
  --model_size tiny `
  --batch_size 16 `
  --num_epochs 10 `
  --use_wandb
```

## ğŸ“ Training

### Basic Training with All Features
```bash
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --model_size tiny \
  --num_epochs 10 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb
```

### Advanced Training with Custom Settings
```bash
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --robot_data data/robot_selection/robot_tasks.json \
  --model_size tiny \
  --batch_size 8 \
  --learning_rate 1e-4 \
  --num_epochs 15 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --hf_repo_name "my-bitgen-model" \
  --use_wandb \
  --wandb_entity babylm-ntust \
  --wandb_project bitgen-training \
  --wandb_tags bitgen multimodal robotics
```

**What Training Provides:**
- âœ… **FLOPS Tracking**: Real-time computational complexity monitoring
- âœ… **CodeCarbon Energy Monitoring**: Energy consumption and carbon footprint tracking
- âœ… **HuggingFace Hub Pushing**: Automatic model upload after every epoch
- âœ… **WandB Logging**: Comprehensive metrics to babylm-ntust team
- âœ… **Advanced Visualizations**: Memory heatmaps, attention patterns, reasoning matrices

## ğŸ¯ Deployment Strategy

**BitGen is designed for this exact workflow:**

### ğŸš€ Training Environment (RTX 4090)
- **High-performance training** with full GPU acceleration
- **Comprehensive monitoring** with FLOPS, energy tracking, and visualizations
- **Advanced metrics** including episodic memory heatmaps and reasoning matrices
- **Automatic model pushing** to HuggingFace Hub after every epoch

### ğŸ“± Inference Environment (Raspberry Pi Zero)
- **Optimized inference** with 1.58-bit quantization for ultra-low power
- **Edge monitoring** with thermal, power, and performance tracking
- **Fast episodic memory operations** (fact editing, selective forgetting)
- **Local knowledge management** without cloud dependencies

## ğŸ”§ Platform-Specific Optimizations

### For Training (RTX 4090):
```bash
# Full-featured training with all monitoring
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --model_size tiny \
  --batch_size 32 \
  --num_epochs 50 \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb
```

### For Inference (Raspberry Pi Zero):
```bash
# Optimized inference with Pi-specific monitoring
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_final.pt \
  --benchmark \
  --show_metrics
```

## ğŸ”® Inference with Performance Metrics

### Interactive Inference with Real-time Monitoring
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --interactive
```

### Comprehensive Benchmark
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --benchmark \
  --num_samples 20 \
  --show_metrics
```

### Single Inference with Metrics
```bash
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt
```

**Inference Metrics Provided:**
- ğŸ¯ **Model Response Throughput**: tokens/sec
- â±ï¸ **Latency**: ms per token and per response
- ğŸ’¾ **Memory Footprint**: RAM usage and peak memory
- âš¡ **Power Consumption**: mW power usage
- ğŸŒ¡ï¸ **Thermal Profile**: CPU temperature monitoring

### Example Inference Output
```
ğŸ“Š COMPREHENSIVE INFERENCE METRICS:
ğŸ¯ PERFORMANCE:
   Model Response Throughput: 3.45 tokens/sec
   Latency per Token: 289.2 ms/token
   Response Time: 1247.8 ms

ğŸ’¾ MEMORY FOOTPRINT:
   Peak RAM Usage: 87.3 MB
   Memory Delta: +12.4 MB

âš¡ POWER & ENERGY:
   Power Consumption: 387.2 mW
   Energy Consumed: 12.4 mJ

ğŸŒ¡ï¸ THERMAL PROFILE:
   CPU Temperature: 62.4Â°C
   Thermal Delta: +2.1Â°C
```

## ğŸ“ˆ Monitoring & Analysis

### System Monitoring
```bash
python bitgen_cli.py monitor --duration 300 --real_time
```

### Results Analysis
```bash
python bitgen_cli.py analyze --results_dir training_monitoring --generate_report
```

## ğŸ—ï¸ Model Architecture

- **Embed Dimensions**: 128D (tiny), 256D (small), 64D (nano)
- **Layers**: 4 layers (tiny), 6 layers (small), 2 layers (nano)
- **Episodic Memory**: 64 memory slots with retrieval and update mechanisms
- **Attention Sinks**: 4 sink tokens for efficient long-sequence processing
- **Cross-Modal Fusion**: Text-image understanding with FIBER architecture
- **Tiny-R1 Reasoning**: Chain-of-thought reasoning with 3-8 LSTM steps
- **Robot Selection**: 5 robot types (Drone, Underwater Robot, Humanoid, Robot with Wheels, Robot with Legs)
  - Top-3 multi-label classification with sigmoid activation
  - 5x5 confusion matrix tracking per epoch
  - Interleaved training: 90% COCO vision-language, 10% robot selection
- **Quantization**: 1.58-bit weights for deployment efficiency

## ğŸ¯ Use Cases

### 1. Multimodal Image Captioning
```python
from src import BitGen

bitgen = BitGen(model_size='tiny')
bitgen.load_checkpoint('checkpoints/best.pt')

# Process image with text
result = bitgen.process_image_and_text('image.jpg', 'Describe the scene')
```

### 2. Robot Task Selection (Top-3 Multi-Label)
```python
# Select top-3 most suitable robots for task/scene
robot_selection = bitgen.select_robots_for_task(
    task_description='Navigate rocky terrain to deliver medical supplies',
    image_path='scene.jpg'  # Optional: scene image for visual context
)

print("ğŸ¤– Top-3 Robot Recommendations:")
for i, (robot, confidence) in enumerate(zip(robot_selection['top_k_robots'], robot_selection['top_k_probs'])):
    print(f"  {i+1}. {robot:<25} (confidence: {confidence:.3f})")

# Output:
# ğŸ¤– Top-3 Robot Recommendations:
#   1. Robot with Legs         (confidence: 0.892)
#   2. Drone                   (confidence: 0.745)
#   3. Robot with Wheels       (confidence: 0.623)
```

### 3. Text Generation with Reasoning
```python
# Generate text with reasoning
response = bitgen.generate_text('<reasoning>The robot should move to</reasoning><answer>')
```

## ğŸ“Š WandB Dashboard (babylm-ntust team)

Training automatically logs to WandB with:
- **Loss curves** and training metrics
- **Episodic memory heatmaps** showing memory utilization
- **Attention pattern visualizations** highlighting important tokens
- **Robot selection matrices** showing reasoning improvement
- **Performance dashboards** with throughput, latency, power consumption
- **Energy tracking** with FLOPS and carbon footprint analysis

Access your runs at: `https://wandb.ai/babylm-ntust/bitgen-training`

## ğŸ¤— HuggingFace Hub Integration

Models are automatically pushed to HuggingFace Hub after every epoch:
- **Auto-generated model names**: `bitgen-{size}-{timestamp}`
- **Detailed model cards** with training metrics and usage instructions
- **Version tracking** with epoch-specific commits
- **Public sharing** for easy collaboration

## ğŸ”§ Configuration Options

### Model Sizes
- **nano**: 64D embed, 2 layers (ultra-lightweight)
- **tiny**: 128D embed, 4 layers (default)
- **small**: 256D embed, 6 layers (higher capacity)

### CLI Commands
- `download`: Download and prepare COCO dataset
- `train`: Train model with comprehensive monitoring
- `inference`: Run inference with performance metrics
- `evaluate`: Evaluate model capabilities
- `deploy`: Deploy for embedded systems
- `monitor`: System performance monitoring
- `analyze`: Analyze training/inference results

## ğŸ“ˆ Performance Targets

### Training Efficiency
- **FLOPS Tracking**: Real-time computational complexity monitoring
- **Energy Monitoring**: CodeCarbon integration for sustainability
- **Memory Optimization**: Episodic memory utilization tracking

### Inference Performance
- **Throughput**: 2-5 tokens/sec on Raspberry Pi
- **Latency**: 200-500ms per token
- **Memory**: <100MB RAM usage
- **Power**: <500mW consumption

## ğŸ¨ Visualizations Available

1. **Memory Access Heatmaps**: Show episodic memory usage patterns
2. **Attention Focus Maps**: Highlight most important tokens per head
3. **Robot Selection Matrices**: Track reasoning accuracy improvement
4. **Performance Dashboards**: Real-time metrics and trends
5. **Energy Efficiency Charts**: FLOPS per mJ, carbon per token

## ğŸ” Troubleshooting

### Common Issues
- **HuggingFace Login**: Ensure `HF_TOKEN` environment variable is set
- **WandB Access**: Verify access to 'babylm-ntust' team
- **Memory Issues**: Reduce batch size or model size for resource-constrained systems
- **Temperature Warnings**: Training automatically pauses if CPU temperature >75Â°C

### Monitoring
All training runs include automatic monitoring and will generate:
- Comprehensive training reports with FLOPS and energy data
- Advanced metrics visualizations saved locally and to WandB
- Model checkpoints with full state information

## ğŸ“ Example Complete Workflow

```bash
# 1. Download data
python bitgen_cli.py download

# 2. Train with full monitoring
python bitgen_cli.py train \
  --coco_data data/coco/validated_coco.json \
  --enable_carbon_tracking \
  --track_flops \
  --push_to_hub \
  --use_wandb

# 3. Run inference with metrics
python bitgen_cli.py inference \
  --model_path checkpoints/bitgen_checkpoint_best.pt \
  --benchmark \
  --show_metrics

# 4. Analyze results
python bitgen_cli.py analyze \
  --results_dir training_monitoring \
  --generate_report
```

The BitGen system provides state-of-the-art multimodal capabilities with comprehensive monitoring, automatic model sharing, and advanced internal analysis - all optimized for efficient deployment and team collaboration.
